from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from copy import deepcopy as dc
import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import pandas as pd
import numpy as np
import yfinance as yf

# ----------- config -----------
PERIOD: str = "1d"
START: str = "1997-01-01"
END: str = "2026-01-01"
SYMBOL: str = "AMZN"
device: str = "cuda:0" if torch.cuda.is_available() else "cpu"
LOOKBACK: int = 7
BATCH_SIZE = 16
LR = 0.001
EPOCHS = 10
# ------------------------------


data = yf.Ticker(SYMBOL).history(period = PERIOD, start = START, end = END)
#data = data[["Open", "High", "Low", "Close", "Volume"]]
data = data[["Close", "Open"]]

def prep_df(data, n_steps):
    global df
    df = dc(data)
    _features = ["Close"]
    for i in range(1, n_steps+1):
      df[f"Close(t-{i})"] = df["Close"].shift(i)
      _features.append(f"Close(t-{i})")
      df.dropna(inplace = True)
    return df[_features]

shifted_data = prep_df(data, LOOKBACK)
scaler = MinMaxScaler(feature_range = (-1, 1))
scaled_data = scaler.fit_transform(shifted_data.to_numpy())

x, y = dc(np.flip(scaled_data[:, 1:], axis = 1)), scaled_data[:, 0]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=False)

x_train = x_train.reshape((-1, LOOKBACK, 1))
x_test = x_test.reshape((-1, LOOKBACK, 1))
y_train = y_train.reshape((-1, 1))
y_test = y_test.reshape((-1, 1))

x_train = torch.tensor(x_train, dtype=torch.float32, device=device)
y_train = torch.tensor(y_train, dtype=torch.float32, device=device)
x_test = torch.tensor(x_test, dtype=torch.float32, device=device)
y_test = torch.tensor(y_test, dtype=torch.float32, device=device)

class TSDataset(Dataset):
  def __init__(self, x, y):
    self.x = x
    self.y = y

  def __len__(self):
    return len(self.x)

  def __getitem__(self, i):
    return self.x[i], self.y[i]

train_data = TSDataset(x_train, y_train)
test_data = TSDataset(x_test, y_test)

train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)
test_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = False)

class Model(nn.Module):
  def __init__(self, input_size, hidden_size, num_stacked_HL):
    super(Model, self).__init__()
    self.hidden_size = hidden_size
    self.num_stacked_HL = num_stacked_HL
    self.model = nn.LSTM(input_size, hidden_size, num_stacked_HL, batch_first = True)
    self.fc = nn.Linear(hidden_size, 1)

  def forward(self, x):
    batch_size = x.size(0)
    h0 = torch.zeros(self.num_stacked_HL, batch_size, self.hidden_size).to(device)
    c0 = torch.zeros(self.num_stacked_HL, batch_size, self.hidden_size).to(device)
    out, _ = self.model(x, (h0, c0))
    out = self.fc(out[:, -1, :])
    return out

model = Model(1, 4, 1)
model.to(device)


loss_func = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = LR)

def train():
  model.train(True)
  print("Epoch: ", epoch + 1)
  running_loss: float = 0.0

  for batch_i, batch in enumerate(train_loader):
    x_batch, y_batch = batch[0].to(device), batch[1].to(device)
    output = model(x_batch)
    loss = loss_func(output, y_batch)
    running_loss += loss.item()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  if batch_i % 100 == 99:
    avg_loss = running_loss / 100
    print(f"Batch {batch_i + 1}, loss: {avg_loss}")
    running_loss = 0.0

def validate():
  model.train(False)
  running_loss: float = 0.0

  for batch_i, batch in enumerate(test_loader):
    x_batch, y_batch = batch[0].to(device), batch[1].to(device)
    with torch.no_grad():
      output = model(x_batch)
      loss = loss_func(output, y_batch)
      running_loss += loss.item()

  avg_loss = running_loss / 100
  print(f"Val Loss: {avg_loss :.3f}")
  print("##########################")
  print()


for epoch in range(EPOCHS):
  train()
  validate()

with torch.no_grad():
  predicted = model(x_train.to(device)).to("cpu").numpy()

y_train_cpu = y_train.to("cpu").numpy()

def reverse_scaling(predicted):
    predicted = predicted.flatten()
    zer = np.zeros((x_train.shape[0], LOOKBACK+1))
    zer[:, 0] = predicted
    zer = scaler.inverse_transform(zer)

    return dc(zer[:, 0])

predicted = reverse_scaling(predicted)
y_train_cpu = reverse_scaling(y_train_cpu)


plt.plot(y_train_cpu, label="Actual Close")
plt.plot(predicted, label="Predicted Close")
plt.xlabel("Day")
plt.ylabel("Close")
plt.legend()
plt.show()
